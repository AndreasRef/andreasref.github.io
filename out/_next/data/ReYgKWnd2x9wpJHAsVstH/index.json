{"pageProps":{"allProjects":[{"title":"ml4a","slug":"ml4a","coverImage":"/assets/images/ml4a_thumb1.jpg","excerpt":"ml4a is a collection of free educational resources devoted to machine learning for artists. I contribute by making code examples, guides and tutorials for applications in openFrameworks and ml5.js.","oneLiner":"Machine Learning for Artists","details":{"year":"2021","technology":"openFrameWorks, ml5.js","flex":{"label":"In collaboration with","value":"Gene Kogan"}}},{"title":"Erasing/Enhancing Essentials","slug":"erasing-enhancing-essentials","coverImage":"/assets/images/erasing-enhancing_thumb.jpg","excerpt":"Erasing/Enhancing Essentials is a project that examines the concept of essence in images by letting algorithms identify and subsequently enhance or erase the most visually important areas of any given image. When exposed to an image a neural network creates a heat map which highlights the areas of the image that are most likely to attract the gaze of a viewer. Using Content-Aware Fill the highlighted areas are either erased, creating uncanny and cold yet evocative scenes, or enhanced, resulting in chaotic dense collages.","oneLiner":"Examining essence in images","details":{"year":"2020","technology":"Tensorflow.js, Adobe Photoshop"}},{"title":"Aagards Glasplader Hacked","slug":"aagaards-glasplader","coverImage":"/assets/images/aagaards_thumb1.jpg","excerpt":"Aagaards Glasplader is a collection of high-resolution portrait photos from 1857-1880. At Hack4dk 2019 I trained a GAN model to generate new portraits of non-existing people and a text generation model to dream up biographies of the generated people.","oneLiner":"Giving life to non-existing people at 24-hour hackathon Hack4dk","details":{"year":"2019","technology":"StyleGAN2, GPT-2, Processing"}},{"title":"fAIry tales","slug":"fairy-tales","coverImage":"/assets/images/fairytales_thumb2.jpg","excerpt":"fAIry tales is a project where fairy tales are automatically generated based on algorithmically detected objects in mundane images. Using images from cocodataset.org I detect everyday objects using the YOLOv3 object detection algorithm. From these objects the title and the beginning of a story is generated and used as prompts for text generation model XLnet.","oneLiner":"Forcing an to AI make up fairy tales from boring picture","details":{"year":"2019","technology":"XLNet, YOLOv3, Processing"}},{"title":"Perler Beads to Landscapes","slug":"perler-beads-to-landscapes","coverImage":"assets/images/perler_thumb.jpg","excerpt":"A little experiment that creates realistic looking digital images from perler beads patterns.","oneLiner":"Creating digital landscape images from perler beads","details":{"year":"2019","technology":"SPADE Landscapes, RunwayML, Processing"}},{"title":"Poems About Things","slug":"poems-about-things","coverImage":"/assets/images/poems_thumb_2.jpg","excerpt":"Poems About Things is a project that generates poetry from everyday objects around us. It consists of a mobile website that constructs quirky sentences about the objects it sees through the users camera feed.","oneLiner":"Generating poetry from the everyday objects around us","details":{"year":"2019","technology":"ml5.js, p5.js, Google Suggest API"}},{"title":"Booksby.ai","slug":"books-by-AI","coverImage":"/assets/images/booksbyai_thumb.jpg","excerpt":"Booksby.ai is an online bookstore that sells science fiction novels written by artificial intelligence. \nThrough training, the artificial intelligence has been exposed to a large number of science fiction books and has learned to generate new ones that mimic the language, style and visual appearance of the books it has read.\nAll books on Booksby.ai are for sale via Amazon.com and can be ordered as printed paperbacks.","oneLiner":"Tired of books written by authors?","details":{"year":"2018","technology":"char-rnn-tensorflow, Progressive Growing of GANs, ml5.js","flex":{"label":"In collaboration with","value":"Mikkel Thybo Loose"}}},{"title":"In Algorithms We Trust","slug":"in_algorithms","coverImage":"/assets/images/in-algo_thumb.jpg","excerpt":"The art piece In Algorithms We Trust invites guests of Roskilde Festival to reflect upon the role of software algorithms while also providing an opportunity to pose a question to Chelsea Manning.","oneLiner":"Machine learning generated fairy tales from mundane images","details":{"year":"2018","technology":"Microsoft Azure Face, Web Technologies'"}},{"title":"Sum of Us","slug":"sum-of-us","coverImage":"/assets/images/sum_thumb.jpg","excerpt":"Emotionally-aware cloud shaped pavilion, where cameras track visitors facial expression and respond with a personalised audio-visual experience, based on the accumulated emotional state for guests at all times.","oneLiner":"Emotionally aware architecture","details":{"year":"2018","technology":"openFrameworks, MaxMSP, Ableton Live","flex":{"label":"In collaboration with","value":"SPACE 10, Sean Lyon, Bo Thorning & Lasse Munk"}}},{"title":"An algorithm watching a movie trailer","slug":"an-algorithm-watching","coverImage":"/assets/images/an-algo_thumb1.jpg","excerpt":"We ran the trailer for The Wolf of Wall Street through an object-detection algorithm that identifies and labels everything on screen. In three separate videos, we essentially see how algorithms watch movies: They label the essentials—a tie, a wine glass, a chair—but leave the specifics out. It’s like visual ad-libs.","oneLiner":"What happens when an object detection algorithm watches a movie trailer?","details":{"year":"2017","technology":"openFrameworks, YOLOv2","flex":{"label":"In collaboration with","value":"Lasse Korsgaard"}}},{"title":"Human-Tree-Machine","slug":"human-tree-machine","coverImage":"/assets/images/human-tree_1.jpg","excerpt":"Collaborative interactive installation that plunges into the speculative and problematic scenario of being able to chat with a one of the festival sites permanent residents: a 40 year old chestnut tree","oneLiner":"Chat with the trees of Roskilde Festival","details":{"year":"2017","technology":"JavaScript, RiveScript"}},{"title":"Sound-Controlled Intergalactic Teddy","slug":"sound-controlled","coverImage":"/assets/images/teddy_thumb.jpg","excerpt":"Sound-Controlled Intergalactic Teddy is an infinite runner game where you use your voice and sounds to control Teddy’s movements. If you need to jump over a green slimy alien you say “Ohhh” and to duck you simply clap your hands.","oneLiner":"Avoiding space monsters with sounds","details":{"year":"2017","technology":"JavaScript","flex":{"label":"In collaboration with","value":"Lasse Korsgaard & Amalie Kvistgaard"}}},{"title":"Teachable Machine","slug":"teachable-machine","coverImage":"/assets/images/teachable-thumb.jpg","excerpt":"Teachable Machine is an experiment that lets anyone explore how machine learning works, in a fun, hands-on way. You can teach a machine to using your camera, live in the browser – no coding required.","oneLiner":"Explore machine learning, live in your browser","details":{"year":"2017","technology":"tensorflow.js","flex":{"label":"In collaboration with","value":"Lasse Korsgaard, Use All Five & Google Creative Lab + PAIR"}}},{"title":"Day&Night","slug":"day-and-night","coverImage":"/assets/images/day-and-night_thumb2.jpg","excerpt":"In the artistic community of Industry City, the Circus Family team built an interactive LED installation, called Day & Night, exclusively designed to fit the exhibition space","oneLiner":"Installation for New York Design Week by Circus Family","details":{"year":"2016","technology":"Processing, Ableton, Resolume","flex":{"label":"In collaboration with","value":"Circus Family"}}},{"title":"Doodle Tunes","slug":"doodle-tunes","coverImage":"/assets/images/doodle_tunes_thumb.jpg","excerpt":"This project lets you turn doodles (drawings) of musical instruments into actual music. A camera looks at your drawing, detects instruments that you have drawn, and begins playing electronic music with those instruments.","oneLiner":"Turning drawings into music","details":{"year":"2016","technology":"openFrameworks, Ableton","flex":{"label":"In collaboration with","value":"Gene Kogan"}}},{"title":"Eye Conductor","slug":"eye-conductor","coverImage":"/assets/images/eye-conductor_thumb.jpg","excerpt":"Eye Conductor is a musical interface that allows people with physical disabilities to play music through eye movements and facial gestures. Using a $99 eye tracker and a regular webcam, Eye Conductor detects the gaze and selected facial movements, thereby enabling people to play any instrument, build beats, sequence melodies or trigger musical effects. The system is open, designed for inclusion and can be customised to fit the physical abilities of whoever is using it.","oneLiner":"Play music with eye movements and facial gestures","details":{"year":"2016","technology":"Processing, FaceOSC, Ableton"}},{"title":"Genetic Paintings","slug":"genetic-paintings","coverImage":"/assets/images/genetic-paintings_thumb.jpg","excerpt":"At Nabi AI Hackathon 2016 in Seoul I presented a working demo for genetic algorithmic paintings that evolve through the process of interactive selection using gaze tracking.","oneLiner":"Paintings that use genetic algorithms to change as you look at them","details":{"year":"2016","technology":"p5.js"}},{"title":"Is it FUNKY?","slug":"is-it-funky","coverImage":"/assets/images/is-it-funky_thumb.jpg","excerpt":"I tried to make a classifier that would distinguish between funky and boring images. And it is of course a bit absurd, what is funky and boring is highly subjective. But machine learning algorithms are increasingly moving into areas that are subjective - so I thought, why not!","oneLiner":"Letting machine learning determine what is funky and boring","details":{"year":"2016","technology":"OpenFrameworks, Wekinator, Ableton"}},{"title":"Painting With Gossip","slug":"painting-with-gossip","coverImage":"/assets/images/painting-with-gossip_thumb.jpg","excerpt":"Painting with Gossip is a website that allows users to draw with the latest headlines from Danish tabloid newspaper EkstraBladet.","oneLiner":"Paint with current tabloid headlines from Danish newspaper EkstraBladet","details":{"year":"2016","technology":"p5.js"}},{"title":"Eyerizon","slug":"eyerizon","coverImage":"/assets/images/eyerizon_thumb1.jpg","excerpt":"Applying speculative and user-centered design principles to complex issues involving social isolation and aging.","oneLiner":"Speculative Face Swapping Project","details":{"year":"2015","technology":"Video","flex":{"label":"In collaboration with","value":"Chelsey Wickmark & Melina Pykkönen"}}},{"title":"Future Self Mirror","slug":"future-self-mirror","coverImage":"/assets/images/futureself_thumb1.jpg","excerpt":"A mirror is the metaphor for self reflection and is an everyday object. We imagined a mirror which gathers data from fitness trackers like fitbits, smartwatches, smartphones, etc and visualises the future health directly on a person’s body while looking at the mirror.","oneLiner":"Augmented Reality Mirror","details":{"year":"2015","technology":"FaceOSC, Processing","flex":{"label":"In collaboration with","value":"Manu Dixit, Riccardo Cereser & Line Birgitte Borgersen"}}},{"title":"Sound Blocks","slug":"sounds-blocks","coverImage":"/assets/images/sound-blocks_thumb1.jpg","excerpt":"Sound Blocks is a tool designed to teach children and adults what sound is made of. The project was shortlisted in the Expression category of the IXDA Interaction Awards and it was developed by John Ferreira, Alejandra Molina, Andreas Refsgaard at the CIID using Arduino.","oneLiner":"Exploring soundwaves through tangible interactions","details":{"year":"2015","technology":"Arduino, Processing, Ableton","flex":{"label":"In collaboration with","value":" John Ferreira & Alejandra Molina"}}},{"title":"Tank Notes","slug":"tank-notes","coverImage":"/assets/images/tank-notes_thumb.jpg","excerpt":"Inspired by the classical Tank Wars game from the 90’s we set out to build a two player synthesizer game with a physical interface during this two day project.","oneLiner":"Musical game using motors and sliders","details":{"year":"2015","technology":"Processing, Ableton, MAX/MSP, Teensy","flex":{"label":"In collaboration with","value":" Håvard Lundberg & Alejandra Molina"}}},{"title":"Video Painter","slug":"video-painter","coverImage":"/assets/images/video-painter_thumb.gif","excerpt":"As a side project during the course Intro to Programming at CIID, I made a Processing sketch that enables anyone to paint with video. I later redid the program with shaders in openFrameworks","oneLiner":"Paint with video in Processing and openFrameworks","details":{"year":"2015","technology":"Processing, openFrameworks"}}]},"__N_SSG":true}